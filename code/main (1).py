# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g_cO-jZf-FD0TqyqLmxDHyCjvKF5lnIL
"""

import pandas as pd
import numpy as np
from sklearn.svm import SVR
import matplotlib.pyplot as plt

dataset = pd.read_csv('Forex.csv')

dataset.head(5)

dataset.shape

num_train = int(dataset.shape[0]*0.7)
num_train

training_set = dataset.iloc[0:num_train, 1].values
training_set.shape

training_set

testing_set = dataset.iloc[num_train:, 1].values
testing_set.shape

testing_set

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set.reshape(-1,1))
testing_set_scaled = sc.fit_transform(testing_set.reshape(-1,1))

# Creating a data structure with 60 timesteps and 1 output
X_train = []
y_train = []
for i in range(60, len(training_set_scaled)):
    X_train.append(training_set_scaled[i-60:i, 0])
    y_train.append(training_set_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)

X_train.shape

X_train.shape[0]

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_train.shape

y_train.shape[0]

y_train = np.reshape(y_train, (y_train.shape[0],1))
y_train.shape

X_test=[]
y_test =[]

for i in range(60, len(testing_set_scaled)):
    X_test.append(testing_set_scaled[i-60:i, 0])
    y_test.append(testing_set_scaled[i,0])
X_test, y_test = np.array(X_test), np.array(y_test)

X_test.shape

X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))
X_test.shape

y_test = np.reshape(y_test, (y_test.shape[0],1))
y_test.shape

import torch

# make training and test sets in torch
x_train = torch.from_numpy(X_train).type(torch.Tensor)
x_test = torch.from_numpy(X_test).type(torch.Tensor)
y_train = torch.from_numpy(y_train).type(torch.Tensor)
y_test = torch.from_numpy(y_test).type(torch.Tensor)

input_dim = 1
hidden_dim = 32
num_layers = 2 
output_dim = 1

class LSTM(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTM, self).__init__()
        # Hidden dimensions
        self.hidden_dim = hidden_dim

        # Number of hidden layers
        self.num_layers = num_layers

        # batch_first=True causes input/output tensors to be of shape
        # (batch_dim, seq_dim, feature_dim)
        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
                # Readout layer
        self.fc = torch.nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

        # Initialize cell state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

        # We need to detach as we are doing truncated backpropagation through time (BPTT)
        # If we don't, we'll backprop all the way to the start even after going through another batch
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))

        # Index hidden state of last time step
        # out.size() --> 100, 32, 100
        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! 
        out = self.fc(out[:, -1, :]) 
        # out.size() --> 100, 10
        return out
    
model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)

loss_fn = torch.nn.MSELoss()

optimiser = torch.optim.Adam(model.parameters(), lr=0.01)
print(model)

# Train model
#####################
num_epochs = 100
hist = np.zeros(num_epochs)

# Number of steps to unroll
seq_dim =60-1  

for t in range(num_epochs):
    # Initialise hidden state
    # Don't do this if you want your LSTM to be stateful
    #model.hidden = model.init_hidden()
    
    # Forward pass
    y_train_pred = model(x_train)

    loss = loss_fn(y_train_pred, y_train)
    if t % 10 == 0 and t !=0:
                print("Epoch ", t, "MSE: ", loss.item())
    hist[t] = loss.item()

    # Zero out gradient, else they will accumulate between epochs
    optimiser.zero_grad()

    # Backward pass
    loss.backward()

    # Update parameters
    optimiser.step()

hist

plt.plot(hist, label="Training loss")
plt.legend()
plt.show()

y_train_pred.shape

plt.plot(y_train_pred.detach().numpy(), label="y_train_pred", color='red')
plt.plot(y_train.detach().numpy(), label="y_train", color='green')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score, r2_score

r2_score(y_train.detach().numpy(), y_train_pred.detach().numpy())

y_test_pred = model(x_test)
y_test_pred.shape

r2_score(y_test.detach().numpy(), y_test_pred.detach().numpy())

plt.plot(y_test_pred.detach().numpy(), label="y_test_pred", color='red')
plt.plot(y_test.detach().numpy(), label="y_test", color='green')
plt.legend()
plt.show()

